{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Basics of LangChain\n",
    "open this [activeloop](https://learn.activeloop.ai/courses/take/langchain/multimedia/46317643-langchain-101-from-zero-to-hero) webpage for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dotenv library to use API keys\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Veda Kashyap\\anaconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.7.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Day 1: \n",
      "- 30 minutes of jogging\n",
      "- 10 minutes of rest\n",
      "- 10 minutes of high-intensity interval training (e.g. sprinting for 30 seconds, then jogging for 30 seconds, repeating for 10 minutes)\n",
      "\n",
      "Day 2: \n",
      "- 20 minutes of walking or running on uneven terrain\n",
      "- 10 minutes of dynamic stretching\n",
      "- 10 minutes of skipping\n",
      "\n",
      "Day 3: \n",
      "- 30 minutes of biking \n",
      "- 10 minutes of rest\n",
      "- 10 minutes of sprinting \n",
      "\n",
      "Day 4: \n",
      "- 20 minutes of swimming\n",
      "- 10 minutes of rest\n",
      "- 10 minutes of jumping jacks\n",
      "\n",
      "Day 5: \n",
      "- 20 minutes of rowing\n",
      "- 10 minutes of rest\n",
      "- 10 minutes of hill sprints\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "prompt = \"\"\"Suggest a personalized workout routine for someone \n",
    "        looking to improve cardiovascular endurance \n",
    "        and prefers outdoor activities.\"\"\"\n",
    "        \n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EcoPure Bottles\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"eco-friendly water bottles\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Memory\n",
    "Provides memory to store Conversation history. `ConversationBufferMemory` acts as a wrapper to `ChatMessageHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Tell me about yourself.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about yourself.\n",
      "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
      "Human: What can you do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about yourself.\n",
      "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
      "Human: What can you do?\n",
      "AI:  I can help you with a variety of tasks, such as scheduling appointments, setting reminders, and providing information. I'm also able to answer questions and provide advice. I'm always learning, so I'm sure I can help you with whatever you need.\n",
      "Human: How can you help me with data analysis?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself.', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\", additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=\" I can help you with a variety of tasks, such as scheduling appointments, setting reminders, and providing information. I'm also able to answer questions and provide advice. I'm always learning, so I'm sure I can help you with whatever you need.\", additional_kwargs={}, example=False), HumanMessage(content='How can you help me with data analysis?', additional_kwargs={}, example=False), AIMessage(content=\" I'm not currently able to help with data analysis, but I'm always learning and expanding my capabilities. I'm sure I'll be able to help you with data analysis in the future.\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# Start the conversation\n",
    "conversation.predict(input=\"Tell me about yourself.\")\n",
    "\n",
    "# Continue the conversation\n",
    "conversation.predict(input=\"What can you do?\")\n",
    "conversation.predict(input=\"How can you help me with data analysis?\")\n",
    "\n",
    "# Display the conversation\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLake\n",
    "Deeplake is a vector store and a datalake fused together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "The dataset is private so make sure you are logged in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Could not automatically map text-embeddings-ada-002 to a tokeniser. Please use `tiktok.get_encoding` to explicitly get the tokeniser you expect.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mh:\\activeloop-langchain\\01-zero_to_hero\\langchain101.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/activeloop-langchain/01-zero_to_hero/langchain101.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m dataset_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhub://\u001b[39m\u001b[39m{\u001b[39;00mactiveloop_org_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mactiveloop_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/activeloop-langchain/01-zero_to_hero/langchain101.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m db \u001b[39m=\u001b[39m DeepLake(dataset_path\u001b[39m=\u001b[39mdataset_path,\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/activeloop-langchain/01-zero_to_hero/langchain101.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m               embedding_function\u001b[39m=\u001b[39membeddings)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/activeloop-langchain/01-zero_to_hero/langchain101.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m db\u001b[39m.\u001b[39;49madd_documents(docs)\n",
      "File \u001b[1;32mc:\\Users\\Veda Kashyap\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain\\vectorstores\\base.py:72\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m     71\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_texts(texts, metadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Veda Kashyap\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:184\u001b[0m, in \u001b[0;36mDeepLake.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mif\u001b[39;00m metadatas \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     metadatas \u001b[39m=\u001b[39m [{}] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(texts))\n\u001b[1;32m--> 184\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39madd(\n\u001b[0;32m    185\u001b[0m     text\u001b[39m=\u001b[39mtexts,\n\u001b[0;32m    186\u001b[0m     metadata\u001b[39m=\u001b[39mmetadatas,\n\u001b[0;32m    187\u001b[0m     embedding_data\u001b[39m=\u001b[39mtexts,\n\u001b[0;32m    188\u001b[0m     embedding_tensor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    189\u001b[0m     embedding_function\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39membedding_function\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    190\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function\u001b[39m.\u001b[39membed_documents,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     return_ids\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    192\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    193\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Veda Kashyap\\anaconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\core\\vectorstore\\deeplake_vectorstore.py:264\u001b[0m, in \u001b[0;36mVectorStore.add\u001b[1;34m(self, embedding_function, embedding_data, embedding_tensor, total_samples_processed, return_ids, num_workers, ingestion_batch_size, **tensors)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    262\u001b[0m utils\u001b[39m.\u001b[39mcheck_length_of_each_tensor(processed_tensors)\n\u001b[1;32m--> 264\u001b[0m dataset_utils\u001b[39m.\u001b[39;49mextend_or_ingest_dataset(\n\u001b[0;32m    265\u001b[0m     processed_tensors\u001b[39m=\u001b[39;49mprocessed_tensors,\n\u001b[0;32m    266\u001b[0m     dataset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset,\n\u001b[0;32m    267\u001b[0m     embedding_function\u001b[39m=\u001b[39;49membedding_function,\n\u001b[0;32m    268\u001b[0m     embedding_data\u001b[39m=\u001b[39;49membedding_data,\n\u001b[0;32m    269\u001b[0m     embedding_tensor\u001b[39m=\u001b[39;49membedding_tensor,\n\u001b[0;32m    270\u001b[0m     ingestion_batch_size\u001b[39m=\u001b[39;49mingestion_batch_size \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mingestion_batch_size,\n\u001b[0;32m    271\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_workers,\n\u001b[0;32m    272\u001b[0m     total_samples_processed\u001b[39m=\u001b[39;49mtotal_samples_processed,\n\u001b[0;32m    273\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m    274\u001b[0m )\n\u001b[0;32m    276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mcommit(allow_empty\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[1;32mc:\\Users\\Veda Kashyap\\anaconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:346\u001b[0m, in \u001b[0;36mextend_or_ingest_dataset\u001b[1;34m(processed_tensors, dataset, embedding_function, embedding_tensor, embedding_data, ingestion_batch_size, num_workers, total_samples_processed, logger)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m embedding_function:\n\u001b[0;32m    343\u001b[0m     \u001b[39mfor\u001b[39;00m func, data, tensor \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    344\u001b[0m         embedding_function, embedding_data, embedding_tensor\n\u001b[0;32m    345\u001b[0m     ):\n\u001b[1;32m--> 346\u001b[0m         embedded_data \u001b[39m=\u001b[39m func(data)\n\u001b[0;32m    347\u001b[0m         embedded_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(embedded_data, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    348\u001b[0m         processed_tensors[tensor] \u001b[39m=\u001b[39m embedded_data\n",
      "File \u001b[1;32mc:\\Users\\Veda Kashyap\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain\\embeddings\\openai.py:305\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \n\u001b[0;32m    295\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[1;32mc:\\Users\\Veda Kashyap\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain\\embeddings\\openai.py:225\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    223\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m    224\u001b[0m indices \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 225\u001b[0m encoding \u001b[39m=\u001b[39m tiktoken\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencoding_for_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[0;32m    226\u001b[0m \u001b[39mfor\u001b[39;00m i, text \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(texts):\n\u001b[0;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m001\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    228\u001b[0m         \u001b[39m# See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[0;32m    229\u001b[0m         \u001b[39m# replace newlines, which can negatively affect performance.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Veda Kashyap\\anaconda3\\envs\\langchain\\lib\\site-packages\\tiktoken\\model.py:70\u001b[0m, in \u001b[0;36mencoding_for_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[39mreturn\u001b[39;00m get_encoding(model_encoding_name)\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m encoding_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m     71\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not automatically map \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m to a tokeniser. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease use `tiktok.get_encoding` to explicitly get the tokeniser you expect.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mreturn\u001b[39;00m get_encoding(encoding_name)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Could not automatically map text-embeddings-ada-002 to a tokeniser. Please use `tiktok.get_encoding` to explicitly get the tokeniser you expect.'"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# instantiate the LLM and Embedding Models\n",
    "llm=OpenAI(model='text-davinci-003',\n",
    "           temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "# create our documents\n",
    "texts=[\"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=0)\n",
    "docs= text_splitter.create_documents(texts)\n",
    "\n",
    "# Create a deeplake dataset that can be later stored in vector store\n",
    "activeloop_org_id = \"vedakashyap\"\n",
    "activeloop_dataset_name = \"langchain_course_zth\"\n",
    "\n",
    "dataset_path = f\"hub://{activeloop_org_id}/{activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path,\n",
    "              embedding_function=embeddings)\n",
    "\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Napoleon Bonaparte was born in 15 August 1769', metadata={}),\n",
       " Document(page_content='Louis XIV was born in 5 September 1638', metadata={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
